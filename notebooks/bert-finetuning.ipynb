!pip -q install datasets transformers evaluate accelerate

from datasets import load_dataset
import re

dataset = load_dataset("stanfordnlp/imdb")


split = dataset["train"].train_test_split(
    test_size=0.1,
    seed=42,
    stratify_by_column="label"
)

train_ds = split["train"]
val_ds   = split["test"]
test_ds  = dataset["test"]


def clean(example):
    example["text"] = re.sub(r"<br\s*/?>", " ", example["text"])
    return example

train_ds = train_ds.map(clean)
val_ds   = val_ds.map(clean)
test_ds  = test_ds.map(clean)

from collections import Counter
print("train:", Counter(train_ds["label"]))
print("val:  ", Counter(val_ds["label"]))
print("test: ", Counter(test_ds["label"]))

from transformers import AutoTokenizer, DataCollatorWithPadding

max_len = 256

def tokenize_datasets(model_ckpt):
    tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)

    def tok(batch):
        return tokenizer(batch["text"], truncation=True, max_length=max_len)

    train_tok = train_ds.map(tok, batched=True, remove_columns=["text"])
    val_tok   = val_ds.map(tok, batched=True, remove_columns=["text"])
    test_tok  = test_ds.map(tok, batched=True, remove_columns=["text"])

    collator = DataCollatorWithPadding(tokenizer=tokenizer)
    return tokenizer, train_tok, val_tok, test_tok, collator

import numpy as np
import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=preds, references=labels)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model_ckpt = "google-bert/bert-base-uncased"

tokenizer, train_tok, val_tok, test_tok, collator = tokenize_datasets(model_ckpt)

model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)

args = TrainingArguments(
    output_dir="bert-imdb",
    eval_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",

    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    warmup_ratio=0.06,

    fp16=True,
    report_to="none",
    seed=42,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_tok,
    eval_dataset=val_tok,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

trainer.train()
trainer.evaluate(test_tok)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model_ckpt = "distilbert/distilbert-base-uncased"

tokenizer, train_tok, val_tok, test_tok, collator = tokenize_datasets(model_ckpt)

model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)

args = TrainingArguments(
    output_dir="distilbert-imdb",
    eval_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",

    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    warmup_ratio=0.06,

    fp16=True,
    report_to="none",
    seed=42,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_tok,
    eval_dataset=val_tok,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

trainer.train()
trainer.evaluate(test_tok)
